{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMleUTC14xxBTXSwtzeJDm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e4tYSy6racce"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n","from sklearn.decomposition import PCA\n","from sklearn.impute import SimpleImputer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n","from mlxtend.frequent_patterns import apriori\n","from mlxtend.frequent_patterns import association_rules"]},{"cell_type":"code","source":["class DataPreprocessor:\n","\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def find_null_values(self):\n","        missing = self.data.isnull().sum()\n","        missing = missing[missing > 0].sort_values(ascending = False)\n","        print(missing)\n","\n","    def remove_duplicates(self):\n","        self.data.drop_duplicates(inplace=True)\n","\n","    def describe_data(self):\n","        print(self.data.describe())\n","\n","    def handle_missing_values_mean(self, columns):\n","        for column in columns:\n","          mode_imputer = SimpleImputer(missing_values = np.nan, strategy= 'mean')\n","          values = data[column].values.reshape(-1,1)\n","          mode_imputer.fit(values)\n","          self.data[column] = mode_imputer.transform(values)\n","\n","    def handle_missing_values_frequent(self, columns):\n","        for column in columns:\n","          mode_imputer = SimpleImputer(missing_values = np.nan, strategy= 'most_frequent')\n","          values = data[column].values.reshape(-1,1)\n","          mode_imputer.fit(values)\n","          self.data[column] = mode_imputer.transform(values)\n","\n","    def remove_outliers(self, columns, threshold=3):\n","        for col in columns:\n","            z_scores = (self.data[col] - self.data[col].mean()) / self.data[col].std()\n","            self.data = self.data[(z_scores.abs() < threshold)]\n","\n","    def calculate_correlation(self):\n","        correlation_matrix = self.data.corr()\n","        print(correlation_matrix)\n","\n","    def label_encoding(self, column):\n","        label_encoder = LabelEncoder()\n","        self.data[column] = label_encoder.fit_transform(self.data[column])\n","        return self.data\n","\n","    def data_normalization(self, columns):\n","        scaler = MinMaxScaler()\n","        self.data[columns] = scaler.fit_transform(self.data[columns])\n","\n","    def data_standardization(self, columns):\n","        scaler = StandardScaler()\n","        self.data[columns] = scaler.fit_transform(self.data[columns])\n","\n","    def apply_PCA(self, columns, n_components):\n","        pca = PCA(n_components=n_components)\n","        pca_result = pca.fit_transform(self.data[columns])\n","        transformed_df = pd.DataFrame(data=pca_result, columns=[f'PC_{i + 1}' for i in range(n_components)])\n","        return transformed_df\n","\n","    def split_data(self, features, target, test_size=0.2, validation_size=0.25, random_state=42):\n","        x = self.data[features]\n","        y = self.data[target]\n","        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state)\n","        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=validation_size, random_state=random_state)\n","        return x_train, x_val, x_test, y_train, y_val, y_test\n"],"metadata":{"id":"McigAmN6agGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Classification:\n","\n","    def knn( X_train, y_train, X_test, y_test):\n","        knn = KNeighborsClassifier(n_neighbors=3)\n","        knn.fit(X_train, y_train)\n","        y_pred = knn.predict(X_test)\n","        accuracy = knn.score(X_test, y_test)\n","        print(f\"Accuracy of k-NN (k=3) on test set: {accuracy:.2f}\")\n","\n","    def decision_tree(X_train, y_train, X_test, y_test):\n","        clf = DecisionTreeClassifier(random_state=42)\n","        clf.fit(X_train, y_train)\n","        accuracy = clf.score(X_test, y_test)\n","        print(f\"Accuracy of Decision Tree on test set: {accuracy:.2f}\")\n","\n","    def random_forest(X_train, y_train, X_test, y_test):\n","        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n","        rf.fit(X_train, y_train)\n","        accuracy = rf.score(X_test, y_test)\n","        print(f\"Accuracy of Random Forest on test set: {accuracy:.2f}\")\n","\n","    def naive_bayes(X_train, y_train, X_test, y_test):\n","        gnb = GaussianNB()\n","        gnb.fit(X_train, y_train)\n","        accuracy = gnb.score(X_test, y_test)\n","        print(f\"Accuracy of Gaussian Naive Bayes on test set: {accuracy:.2f}\")\n","\n","    def svm(X_train, y_train, X_test, y_test):\n","        svm = SVC(kernel='linear', random_state=42)\n","        svm.fit(X_train, y_train)\n","        accuracy = svm.score(X_test, y_test)\n","        print(f\"Accuracy of SVM on test set: {accuracy:.2f}\")"],"metadata":{"id":"xPhMZGoCjTtM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Clustering:\n","\n","  def kmeans(data, n_clusters):\n","      kmeans = KMeans(n_clusters=n_clusters)\n","      kmeans.fit(data)\n","      cluster_labels = kmeans.labels_\n","      clustered_data = data.copy()\n","      clustered_data['Cluster'] = cluster_labels\n","      return clustered_data\n","\n","  def dbscan(data, eps, min_samples):\n","      dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n","      cluster_labels = dbscan.fit_predict(data)\n","      clustered_data = data.copy()\n","      clustered_data['Cluster'] = cluster_labels\n","      return clustered_data\n","\n","  def agglomerative_clustering(data, n_clusters):\n","      agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n","      cluster_labels = agg_clustering.fit_predict(data)\n","      clustered_data = data.copy()\n","      clustered_data['Cluster'] = cluster_labels\n","      return clustered_data"],"metadata":{"id":"4ozb0nGSpzef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Rule_Mining:\n","\n","  def perform_apriori(data, min_support, min_threshold):\n","      frequent_itemsets = apriori(data, min_support=min_support, use_colnames=True)\n","      rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=min_threshold)\n","      return rules"],"metadata":{"id":"8Hlih0AAuJjn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading the data\n","data = pd.read_csv(\"Wines.csv\")\n","data"],"metadata":{"id":"PlzxrGV1whN3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing unnamed columns\n","data = data.iloc[:,1:14]\n","data"],"metadata":{"id":"59adcWgLw52b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the data in Preprocessor class\n","preprocessor = DataPreprocessor(data)"],"metadata":{"id":"qUznv55vxBjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for Null values\n","preprocessor.find_null_values()"],"metadata":{"id":"4O_k0tRTxO5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Imputing the Null Values for numerical columns\n","preprocessor.handle_missing_values_mean(['Chlorides', 'Volatile_Acidity', 'Sugar', 'Alcohol', 'Citric Acid'])\n","\n","# Imputing the Null values for categorical columns\n","preprocessor.handle_missing_values_frequent(['Color'])"],"metadata":{"id":"3lntCNzWxtdu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check for null values after imputing\n","preprocessor.find_null_values()"],"metadata":{"id":"LAMVhf84ya41"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove Duplicate Rows\n","preprocessor.remove_duplicates()"],"metadata":{"id":"27NCiU6-yzF6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Details of data\n","preprocessor.describe_data()"],"metadata":{"id":"CBa40mPozBnb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Correlation Analysis\n","preprocessor.calculate_correlation()"],"metadata":{"id":"qmjwifsNzMLp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove Outliers\n","column_names_array = data.columns.tolist()\n","column_names_array.remove('Color')\n","column_names_array.remove('Quality')\n","\n","preprocessor.remove_outliers(column_names_array)"],"metadata":{"id":"WitF8O63zq14"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform Label Encoding in the Categorical column named Color\n","data = preprocessor.label_encoding('Color')"],"metadata":{"id":"SNBAZ-b40QEO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Storing the Target column seperately and removing it from the dataset for performing preprocessing\n","Quality_column = data.pop('Quality')"],"metadata":{"id":"Doi6eJD51fJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing Normalization and Standardization\n","preprocessor.data_normalization(column_names_array)\n","preprocessor.data_standardization(column_names_array)"],"metadata":{"id":"mWBZ2GlZ5HvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Applying Principal Component Analysis(PCA)\n","new_data = preprocessor.apply_PCA(['Alcohol', 'Volatile_Acidity'], 2)\n","new_data"],"metadata":{"id":"bsO7hCYA5lG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding back the Quality column in dataset\n","data['Quality'] = Quality_column"],"metadata":{"id":"2UCCNkJV-Bix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Partioning\n","column_names_array = data.columns.tolist()\n","column_names_array.remove('Quality')\n","features = column_names_array\n","\n","x_train, x_val, x_test, y_train, y_val, y_test = preprocessor.split_data(features=features, target='Quality')"],"metadata":{"id":"O6xFI6cg-Tw0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing KNN Classification\n","Classification.knn(x_train, y_train, x_test, y_test)"],"metadata":{"id":"zBA1dqmn-ZBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing Decision Tree Classifier\n","Classification.decision_tree(x_train, y_train, x_test, y_test)"],"metadata":{"id":"UuW_V7d6_SqO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing Random Forest Classifier\n","Classification.random_forest(x_train, y_train, x_test, y_test)"],"metadata":{"id":"xOY3PanFCIMX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing Naive Bayes Classifier\n","Classification.naive_bayes(x_train, y_train, x_test, y_test)"],"metadata":{"id":"CJBkXJtCCQFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing Support Vector Machines(SVM) Classifier\n","Classification.svm(x_train, y_train, x_test, y_test)"],"metadata":{"id":"yfgxJXHWCT0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing K Means clustering\n","clustered_data = Clustering.kmeans(data,3)\n","clustered_data"],"metadata":{"id":"Xsu8det_Cf2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing DBSCAN\n","eps = 0.5\n","min_samples = 5\n","clustered_data = Clustering.dbscan(data, eps, min_samples)\n","clustered_data"],"metadata":{"id":"WJvGt2OVCp5f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing agglomerative clustering\n","n_clusters = 3\n","Clustering.agglomerative_clustering(data, n_clusters)"],"metadata":{"id":"oV7F4fRIDRJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Performing apriori algorithm (Association Rule Mining)\n","\n","data_encoded = pd.get_dummies(data)\n","# Binarize the dataset based on some threshold or specific conditions\n","data_binarized = data_encoded.applymap(lambda x: 1 if x >= 1 else 0)\n","\n","min_supp = 0.1\n","min_thresh = 0.7\n","association_rules = Rule_Mining.perform_apriori(data_binarized, min_support=min_supp, min_threshold=min_thresh)\n","association_rules"],"metadata":{"id":"6LW5bLlFEGvY"},"execution_count":null,"outputs":[]}]}